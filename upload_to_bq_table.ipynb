{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-29T15:00:50.706758Z",
     "start_time": "2018-10-29T15:00:50.522195Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-29T15:00:52.961617Z",
     "start_time": "2018-10-29T15:00:52.380861Z"
    }
   },
   "outputs": [],
   "source": [
    "# to enable the google-cloud sdk \"module\", install the google-cloud-bigquery client library in shell/command prompt\n",
    "# pip install --upgrade google-cloud-bigquery\n",
    "# check google.cloud.bigquery version with: \n",
    "# pip show google-cloud\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-29T15:58:36.690739Z",
     "start_time": "2018-10-29T15:58:36.688291Z"
    }
   },
   "outputs": [],
   "source": [
    "# check in which path your current working directory is\n",
    "# os.getcwd()import datetime as datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as datetime\n",
    "client = bigquery.Client.from_service_account_json('<placeholder>') #populate with your bigQuery service account\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "PROJECT=\"\"\n",
    "DATASET=\"\"\n",
    "QUERY=\"\"\n",
    "START_DATE = (now - datetime.timedelta(days=7)).strftime(\"%Y-%m-%d\")\n",
    "END_DATE = (now - datetime.timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "base_path = \"\"\n",
    "filename=\"london_crime.sql\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analytics_summit_20181108\n"
     ]
    }
   ],
   "source": [
    "# target table id\n",
    "t_id = 'analytics_summit_'+END_DATE.replace(\"-\",\"\")\n",
    "print(t_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-02\n",
      "2018-11-08\n"
     ]
    }
   ],
   "source": [
    "bq_date=now.strftime(\"%Y-%m-%d\")\n",
    "print(START_DATE)\n",
    "print(END_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add function for BQ client\n",
    "def bigquery_job ():\n",
    "    #query config\n",
    "    path_to_file = os.path.join(base_path, filename)\n",
    "    fd = open(path_to_file , 'r')\n",
    "    q=fd.read()\n",
    "    query = q.format(start_date=START_DATE,end_date=END_DATE,dataset=DATASET,query=QUERY)\n",
    "    #print(query)\n",
    "    #connection config\n",
    "    query_job = bigquery.QueryJobConfig()\n",
    "    query_job.use_legacy_sql = False\n",
    "    \n",
    "    start=datetime.datetime.now()\n",
    "    print('start: ' + str(start))\n",
    "    query = client.query(query, job_config=query_job)\n",
    "    result = query.result(timeout=None)\n",
    "    dataframe = result.to_dataframe()\n",
    "    bytesProcessed = query.total_bytes_processed\n",
    "    jobId = query.job_id\n",
    "    end=datetime.datetime.now()\n",
    "    print('end: ' + str(end))\n",
    "    print('duration: ' + str((end-start)))\n",
    "    print('processed bytes: '+ str(bytesProcessed) \n",
    "          + ', hexa GB: ' + str(float(bytesProcessed/10**9))\n",
    "          + ', in GiB: ' + str(float(bytesProcessed/2**30)) \n",
    "          + ', in TB: ' + str(float(bytesProcessed/2**40)))\n",
    "    print('job ID: '+jobId)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 2018-11-09 22:39:23.460315\n",
      "end: 2018-11-09 22:39:29.954995\n",
      "duration: 0:00:06.494680\n",
      "processed bytes: 629125272, hexa GB: 0.629125272, in GiB: 0.5859185680747032, in TB: 0.0005721861016354524\n",
      "job ID: a9d0045e-b19c-440d-ba73-982b4680ab2e\n"
     ]
    }
   ],
   "source": [
    "sample = bigquery_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>borough</th>\n",
       "      <th>major_category</th>\n",
       "      <th>total_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Westminster</td>\n",
       "      <td>Theft and Handling</td>\n",
       "      <td>27520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Camden</td>\n",
       "      <td>Theft and Handling</td>\n",
       "      <td>14088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lambeth</td>\n",
       "      <td>Theft and Handling</td>\n",
       "      <td>13155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Southwark</td>\n",
       "      <td>Theft and Handling</td>\n",
       "      <td>12946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Islington</td>\n",
       "      <td>Theft and Handling</td>\n",
       "      <td>12077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       borough      major_category  total_value\n",
       "0  Westminster  Theft and Handling        27520\n",
       "1       Camden  Theft and Handling        14088\n",
       "2      Lambeth  Theft and Handling        13155\n",
       "3    Southwark  Theft and Handling        12946\n",
       "4    Islington  Theft and Handling        12077"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print a preview of the dataframe's first rows\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF running on local machine or not having installed any further packages, \n",
    "# I installed 'pyarrow' to have the insert job running, as proposed by the resulting error if not done so.\n",
    "# Alternatively it proposed 'fastparquet'\n",
    "# here is the code to drop in the terminal for the pyarrow package (installation worked with Python 3.7 Windows 64-bit):\n",
    "#     conda install -c conda-forge pyarrow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to write query to a table in above defined dataset\n",
    "def bigquery_insert ():\n",
    "    project_id ='<placeholder>'\n",
    "    dataset_id = 'analytics_summit'\n",
    "    table_id = t_id\n",
    "    #client = bigquery.Client.from_service_account_json('')   #populate with your bigQuery service account\n",
    "\n",
    "    #query config\n",
    "    path_to_file = os.path.join(base_path, filename)\n",
    "    fd = open(path_to_file , 'r')\n",
    "    q=fd.read()\n",
    "    query = q.format(start_date=START_DATE,end_date=END_DATE,dataset=DATASET,query=QUERY)\n",
    "\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    # Set the destination table\n",
    "    client.project = project_id\n",
    "    table_ref = client.dataset(dataset_id).table(table_id)\n",
    "    job_config.destination = table_ref\n",
    "\n",
    "    \n",
    "    trial_job = client.load_table_from_dataframe(sample, table_ref)\n",
    "\n",
    "    trial_job.result()  # Waits for the query to finish\n",
    "    print('Query results loaded to table {}'.format(table_ref.path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query results loaded to table /projects/tough-zoo-22/datasets/analytics_summit/tables/analytics_summit_20181108\n"
     ]
    }
   ],
   "source": [
    "bigquery_insert()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
